from keras import regularizers
from keras.applications import DenseNet121
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten
from keras.layers import GlobalAveragePooling2D, Input

from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.preprocessing import image
from keras.optimizers import Adam
from tensorflow.keras.metrics import *
from sklearn.preprocessing import MultiLabelBinarizer, OrdinalEncoder

from PIL import Image
import numpy as np
import pandas as pd
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm 
import cv2 
import os



movies_train = pd.read_csv('dataset_cleaned/movies_train_update.DAT', engine='python',
                         sep=',', names=['movieid', 'title', 'genre'], encoding='latin-1', index_col=False)
movies_test = pd.read_csv('dataset_cleaned/movies_test_update.DAT', engine='python',
                         sep=',', names=['movieid', 'title', 'genre'], encoding='latin-1', index_col=False)
movies_train['genre'] = movies_train.genre.str.split('|')
movies_test['genre'] = movies_test.genre.str.split('|')


genres = ['Action', 'Adventure', 'Animation', "Children's", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']
for genre in genres:
    movies_train[genre] = movies_train['genre'].apply(lambda x: 1 if genre in x else 0)
    movies_test[genre] = movies_test['genre'].apply(lambda x: 1 if genre in x else 0)


movies_test


movies_train



genre_df = pd.DataFrame(movies_train['genre'].explode())
count_genre = genre_df['genre'].value_counts()
count_genre


plt.figure(figsize=(12, 6))
count_genre.plot(kind= 'bar', figsize= (14, 6))
plt.title('Genre Distribution')
plt.xlabel('Genre')
plt.ylabel('Count')
plt.show()


SIZE = 200
sources_dir = 'ml1m-images/'
def preprocessing(data):
  X_dataset = []
  for i in tqdm(range(data.shape[0])):
    img = image.load_img(sources_dir +str(data['movieid'][i])+'.jpg', target_size=(SIZE,SIZE,3))
    img = image.img_to_array(img)
    img = img/255.
    X_dataset.append(img)
  X = np.array(X_dataset)
  mlb = MultiLabelBinarizer()
  mlb.fit(data['genre'])
  # transform target variable
  y = mlb.transform(data['genre'])
  return X, y


x_train, y_train = preprocessing(movies_train)
x_test, y_test = preprocessing(movies_test)


import keras.backend as K
def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))


def basemodel(num_classes, mode, learning_rate):
    base_model = DenseNet121(weights = 'dataset_cleaned/densenet121_notop.h5',include_top=False, input_shape=(SIZE, SIZE, 3))
    # Add a global spatial average pooling layer
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    # Add a fully-connected layer
    x = Dense(32, activation='relu', 
              kernel_regularizer=regularizers.l2(0.01))(x)
    x = Dropout(0.5)(x)
    # Add a logistic layer for the 18 classes
    predictions = Dense(num_classes, activation='sigmoid')(x)  # Sigmoid activation for multilabel classification
    # Model to be trained
    model = Model(inputs=base_model.input, outputs=predictions)
    # Đóng băng các layer của base model
    for layer in base_model.layers:
        layer.trainable = mode
    model.compile(optimizer=Adam(lr = learning_rate), loss='binary_crossentropy', 
                  metrics=[f1_m,precision_m, recall_m])
    return model


def mode(mode):
    if mode == 0:
        model = basemodel(18, False, 0.001)
    elif mode == 1:
        model = basemodel(18, True, 0.00001)
    return model
# model.summary()


es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)
mc = ModelCheckpoint('dataset_cleaned/best_weights_32.h5', 
                         monitor='val_f1_m', verbose=1, save_best_only=True, 
                         mode='max', save_weights_only=True)


file_path = 'dataset_cleaned/best_weights_32.h5'
model = mode(0)
if os.path.exists(file_path):
    model.load_weights(file_path)

history = model.fit(x_train, y_train,verbose = 1, epochs=50, validation_data=(x_test, y_test),
                    batch_size = 64)

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title(f'Training and validation loss ')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

f1 = history.history['f1_m']
val_f1 = history.history['val_f1_m']

plt.plot(epochs, f1, 'y', label='Training - F1Score')
plt.plot(epochs, val_f1, 'r', label='Validation - F1Score')
plt.title(f'Training and validation - F1Score ')
plt.xlabel('Epochs')
plt.ylabel('F1Score')
plt.legend()
plt.show()


predictions = model.predict(x_test)
sorted_prediction_ids = np.argsort(-predictions, axis=1)


enc = OrdinalEncoder()
enc.fit_transform(genre_df[['genre']])


vectors_labels_test = movies_test.drop(columns = ['movieid', 'title', 'genre'], axis = 1)


def get_column_names(row):
    return list(vectors_labels_test.columns[row == 1])
vectors_labels_test_new=vectors_labels_test.apply(get_column_names,axis=1).tolist()


top_5_prediction_ids = sorted_prediction_ids[:,:5]
original_shape = top_5_prediction_ids.shape
top_5_predictions = enc.inverse_transform(top_5_prediction_ids.reshape(-1, 1))
top_5_predictions = top_5_predictions.reshape(original_shape)
top_5_predictions[:10] # Spot check our first 10 values


def apk(actual, predicted, k):
    if len(predicted)>k:
        predicted = predicted[:k]
    score = 0.0
    num_hits = 0.0
    for i,p in enumerate(predicted):
        if p in actual and p not in predicted[:i]:
            num_hits += 1.0
            score += num_hits / (i+1.0)
    if not actual:
        return 0.0
    return score / min(len(actual), k)

def mapk(actual, predicted, k):
    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])


print('map@k score = {:.3}'.format(mapk(vectors_labels_test_new,top_5_predictions,k = 5)))


import random

i = random.randint(0, 650)

index = movies_test['movieid'][i]
img = image.load_img(f'ml1m-images/{index}.jpg', target_size=(SIZE,SIZE,3))

img = image.img_to_array(img)
img = img/255.
plt.imshow(img)
print(movies_test['genre'][i])


#%%
proba = model.predict(img.reshape(1,SIZE,SIZE,3))
top_5 = np.argsort(proba[0])[:-11:-1]
for i in range(5):
    print("{}".format(genres[top_5[i]])+" ({:.3})".format(proba[0][top_5[i]]))
plt.imshow(img)
#%%



